- title: "Over-squashing in Spatiotemporal Graph Neural Networks"
  links:
    paper: https://arxiv.org/abs/2506.15507
  venue: Preprint
  short_venue: Preprint
  year: 2025
  thumbnail: 2506.15507.webp
  authors:
    - me
    - name: Jacob
      surname: Bamberger
    - name: Cesare
      surname: Alippi
    - name: Michael M.
      surname: Bronstein
  keywords:
    - spatiotemporal graphs
    - forecasting
    - over-squashing
  abstract: >
    Graph Neural Networks (GNNs) have achieved remarkable success across various domains. However, recent theoretical advances have identified fundamental limitations in their information propagation capabilities, such as over-squashing, where distant nodes fail to effectively exchange information. While extensively studied in static contexts, this issue remains unexplored in Spatiotemporal GNNs (STGNNs), which process sequences associated with graph nodes. Nonetheless, the temporal dimension amplifies this challenge by increasing the information that must be propagated. In this work, we formalize the spatiotemporal over-squashing problem and demonstrate its distinct characteristics compared to the static case. Our analysis reveals that counterintuitively, convolutional STGNNs favor information propagation from points temporally distant rather than close in time. Moreover, we prove that architectures that follow either time-and-space or time-then-space processing paradigms are equally affected by this phenomenon, providing theoretical justification for computationally efficient implementations. We validate our findings on synthetic and real-world datasets, providing deeper insights into their operational dynamics and principled guidance for more effective designs.
  bibtex: >
    @article{marisca2025oversquashing,
      title     = {Over-squashing in Spatiotemporal Graph Neural Networks},
      author    = {Ivan Marisca and Jacob Bamberger and Cesare Alippi and Michael M. Bronstein},
      year      = {2025},
      journal   = {arXiv preprint arXiv:2506.15507},
      url       = {https://arxiv.org/abs/2506.15507},
    }
- title: 'PeakWeather: MeteoSwiss Weather Station Measurements for Spatiotemporal Deep Learning'
  links:
    paper: https://arxiv.org/abs/2506.13652
    code: https://github.com/Graph-Machine-Learning-Group/peakweather-wind-forecasting
  venue: Preprint
  short_venue: Preprint
  year: 2025
  authors:
    - name: Daniele
      surname: Zambon
    - name: Michele
      surname: Cattaneo
    - me
    - name: Jonas
      surname: Bhend
    - name: Daniele
      surname: Nerini
    - name: Cesare
      surname: Alippi
  first_authors: 2
  thumbnail: 2402.10634.jpg
  keywords:
    - spatiotemporal graphs
    - forecasting
    - benchmark
    - weather forecasting
  abstract: >
    In forecasting multiple time series, accounting for the individual features of each sequence can be challenging. To address this, modern deep learning methods for time series analysis combine a shared (global) model with local layers, specific to each time series, often implemented as learnable embeddings. Ideally, these local embeddings should encode meaningful representations of the unique dynamics of each sequence. However, when these are learned end-to-end as parameters of a forecasting model, they may end up acting as mere sequence identifiers. Shared processing blocks may then become reliant on such identifiers, limiting their transferability to new contexts. In this paper, we address this issue by investigating methods to regularize the learning of local learnable embeddings for time series processing. Specifically, we perform the first extensive empirical study on the subject and show how such regularizations consistently improve performance in widely adopted architectures. Furthermore, we show that methods attempting to prevent the co-adaptation of local and global parameters by means of embeddings perturbation are particularly effective in this context. In this regard, we include in the comparison several perturbation-based regularization methods, going as far as periodically resetting the embeddings during training. The obtained results provide an important contribution to understanding the interplay between learnable local parameters and shared processing layers: a key challenge in modern time series processing models and a step toward developing effective foundation models for time series.
  bibtex: >
    @misc{zambon2025peakweather,
      title={PeakWeather: MeteoSwiss Weather Station Measurements for Spatiotemporal Deep Learning}, 
      author={Zambon, Daniele and Cattaneo, Michele and Marisca, Ivan and Bhend, Jonas and Nerini, Daniele and Alippi, Cesare},
      year={2025},
      eprint={2506.13652},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.13652}, 
    }
- title: Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling
  links:
    paper: https://arxiv.org/abs/2402.10634
    code: https://github.com/marshka/hdtts
  venue: International Conference on Machine Learning
  short_venue: ICML
  year: 2024
  authors:
    - me
    - name: Cesare
      surname: Alippi
    - name: Filippo Maria
      surname: Bianchi
  thumbnail: 2402.10634.jpg
  keywords:
    - spatiotemporal graphs
    - forecasting
  abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate the forecasts. Our approach outperforms state-of-the-art methods on synthetic and real-world benchmarks under different missing data distributions, particularly in the presence of contiguous blocks of missing values.
- title: Graph Deep Learning for Time Series Forecasting
  links:
    paper: https://dl.acm.org/doi/10.1145/3742784
  venue: ACM Computing Surverys
  short_venue: ACM CSUR
  year: 2025
  authors:
    - name: Andrea
      surname: Cini
    - me
    - name: Daniele
      surname: Zambon
    - name: Cesare
      surname: Alippi
  thumbnail: 2310.15978.webp
  keywords:
    - spatiotemporal graphs
    - forecasting
  abstract: Graph-based deep learning methods have become popular tools to process collections of correlated time series. Differently from traditional multivariate forecasting methods, neural graph-based predictors take advantage of pairwise relationships by conditioning forecasts on a (possibly dynamic) graph spanning the time series collection. The conditioning can take the form of an architectural inductive bias on the neural forecasting architecture, resulting in a family of deep learning models called spatiotemporal graph neural networks. Such relational inductive biases enable the training of global forecasting models on large time-series collections, while at the same time localizing predictions w.r.t. each element in the set (i.e., graph nodes) by accounting for local correlations among them (i.e., graph edges). Indeed, recent theoretical and practical advances in graph neural networks and deep learning for time series forecasting make the adoption of such processing frameworks appealing and timely. However, most of the studies in the literature focus on proposing variations of existing neural architectures by taking advantage of modern deep learning practices, while foundational and methodological aspects have not been subject to systematic investigation. To fill the gap, this paper aims to introduce a comprehensive methodological framework that formalizes the forecasting problem and provides design principles for graph-based predictive models and methods to assess their performance. At the same time, together with an overview of the field, we provide design guidelines, recommendations, and best practices, as well as an in-depth discussion of open challenges and future research directions.
- title: Taming Local Effects in Graph-based Spatiotemporal Forecasting
  links:
    paper: https://arxiv.org/abs/2302.04071
    code: https://github.com/Graph-Machine-Learning-Group/taming-local-effects-stgnns
  venue: Advances in Neural Information Processing Systems
  short_venue: NeurIPS
  year: 2023
  authors:
    - name: Andrea
      surname: Cini
    - me
    - name: Daniele
      surname: Zambon
    - name: Cesare
      surname: Alippi
  first_authors: 2
  thumbnail: 2302.04071.jpg
  keywords:
    - spatiotemporal graphs
    - forecasting
    - embeddings
  abstract: Spatiotemporal graph neural networks have shown to be effective in time series forecasting applications, achieving better performance than standard univariate predictors in several settings. These architectures take advantage of a graph structure and relational inductive biases to learn a single (global) inductive model to predict any number of the input time series, each associated with a graph node. Despite the gain achieved in computational and data efficiency w.r.t. fitting a set of local models, relying on a single global model can be a limitation whenever some of the time series are generated by a different spatiotemporal stochastic process. The main objective of this paper is to understand the interplay between globality and locality in graph-based spatiotemporal forecasting, while contextually proposing a methodological framework to rationalize the practice of including trainable node embeddings in such architectures. We ascribe to trainable node embeddings the role of amortizing the learning of specialized components. Moreover, embeddings allow for 1) effectively combining the advantages of shared message-passing layers with node-specific parameters and 2) efficiently transferring the learned model to new node sets. Supported by strong empirical evidence, we provide insights and guidelines for specializing graph-based models to the dynamics of each time series and show how this aspect plays a crucial role in obtaining accurate predictions.
- title: Scalable Spatiotemporal Graph Neural Networks
  links:
    paper: https://arxiv.org/abs/2209.06520
    code: https://github.com/Graph-Machine-Learning-Group/sgp
  venue: Proceedings of the AAAI conference on artificial intelligence
  short_venue: AAAI
  year: 2023
  authors:
    - name: Andrea
      surname: Cini
    - me
    - name: Filippo Maria
      surname: Bianchi
    - name: Cesare
      surname: Alippi
  first_authors: 2
  thumbnail: 2209.06520.jpg
  keywords:
    - spatiotemporal graphs
    - forecasting
    - reservoir computing
  abstract:
    Neural forecasting of spatiotemporal time series drives both research
    and industrial innovation in several relevant application domains. Graph neural
    networks (GNNs) are often the core component of the forecasting architecture.
    However, in most spatiotemporal GNNs, the computational complexity scales up to
    a quadratic factor with the length of the sequence times the number of links in
    the graph, hence hindering the application of these models to large graphs and
    long temporal sequences. While methods to improve scalability have been proposed
    in the context of static graphs, few research efforts have been devoted to the
    spatiotemporal case. To fill this gap, we propose a scalable architecture that
    exploits an efficient encoding of both temporal and spatial dynamics. In particular,
    we use a randomized recurrent neural network to embed the history of the input
    time series into high-dimensional state representations encompassing multi-scale
    temporal dynamics. Such representations are then propagated along the spatial
    dimension using different powers of the graph adjacency matrix to generate node
    embeddings characterized by a rich pool of spatiotemporal features. The resulting
    node embeddings can be efficiently pre-computed in an unsupervised manner, before
    being fed to a feed-forward decoder that learns to map the multi-scale spatiotemporal
    representations to predictions. The training procedure can then be parallelized
    node-wise by sampling the node embeddings without breaking any dependency, thus
    enabling scalability to large networks. Empirical results on relevant datasets
    show that our approach achieves results competitive with the state of the art,
    while dramatically reducing the computational burden.
- title: "Learning to Reconstruct Missing Data from Spatiotemporal Graphs with Sparse Observations"
  links:
    paper: https://arxiv.org/abs/2205.13479
    code: https://github.com/Graph-Machine-Learning-Group/spin
  venue: Advances in Neural Information Processing Systems
  short_venue: NeurIPS
  year: 2022
  authors:
    - me
    - name: Andrea
      surname: Cini
    - name: Cesare
      surname: Alippi
  first_authors: 2
  thumbnail: 2205.13479.jpg
  keywords:
    - spatiotemporal graphs
    - imputation
  abstract:
    Modeling multivariate time series as temporal signals over a (possibly
    dynamic) graph is an effective representational framework that allows for developing
    models for time series analysis. In fact, discrete sequences of graphs can be
    processed by autoregressive graph neural networks to recursively learn representations
    at each discrete point in time and space. Spatiotemporal graphs are often highly
    sparse, with time series characterized by multiple, concurrent, and even long
    sequences of missing data, e.g., due to the unreliable underlying sensor network.
    In this context, autoregressive models can be brittle and exhibit unstable learning
    dynamics. The objective of this paper is, then, to tackle the problem of learning
    effective models to reconstruct, i.e., impute, missing data points by conditioning
    the reconstruction only on the available observations. In particular, we propose
    a novel class of attention-based architectures that, given a set of highly sparse
    discrete observations, learn a representation for points in time and space by
    exploiting a spatiotemporal diffusion architecture aligned with the imputation
    task. Representations are trained end-to-end to reconstruct observations w.r.t.
    the corresponding sensor and its neighboring nodes. Compared to the state of the
    art, our model handles sparse data without propagating prediction errors or requiring
    a bidirectional model to encode forward and backward time dependencies. Empirical
    results on representative benchmarks show the effectiveness of the proposed method.
- title: "Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks"
  links:
    paper: https://arxiv.org/abs/2108.00298
    code: https://github.com/Graph-Machine-Learning-Group/grin
  venue: International Conference on Learning Representations
  short_venue: ICLR
  year: 2022
  authors:
    - name: Andrea
      surname: Cini
    - me
    - name: Cesare
      surname: Alippi
  first_authors: 2
  thumbnail: 2108.00298.jpg
  keywords:
    - spatiotemporal graphs
    - imputation
  abstract:
    Dealing with missing values and incomplete time series is a labor-intensive,
    tedious, inevitable task when handling data coming from real-world applications.
    Effective spatio-temporal representations would allow imputation methods to reconstruct
    missing temporal data by exploiting information coming from sensors at different
    locations. However, standard methods fall short in capturing the nonlinear time
    and space dependencies existing within networks of interconnected sensors and
    do not take full advantage of the available - and often strong - relational information.
    Notably, most state-of-the-art imputation methods based on deep learning do not
    explicitly model relational aspects and, in any case, do not exploit processing
    frameworks able to adequately represent structured spatio-temporal data. Conversely,
    graph neural networks have recently surged in popularity as both expressive and
    scalable tools for processing sequential data with relational inductive biases.
    In this work, we present the first assessment of graph neural networks in the
    context of multivariate time series imputation. In particular, we introduce a
    novel graph neural network architecture, named GRIN, which aims at reconstructing
    missing data in the different channels of a multivariate time series by learning
    spatio-temporal representations through message passing. Empirical results show
    that our model outperforms state-of-the-art methods in the imputation task on
    relevant real-world benchmarks with mean absolute error improvements often higher
    than 20%.
